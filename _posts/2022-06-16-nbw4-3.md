---
title:  "내일배움단 Web4.3"
categories: SpartaWeb
toc: true
toc_sticky: true
sidebar:
    nav: docs
---

# [웹개발 종합반] 4주 3일차 개발일지(4주 10-16강, 숙제)

<br>

## 💻TIL Web.403

## '나홀로 메모장' 프로젝트

## 1. 프로젝트 세팅

이번 프로젝트에서는, 이전에 만든 ['나홀로 링크 메모장' 페이지][1]를 서버와 연동해 입력한 URL과 리뷰가 카드 형태로 붙여질 수 있도록 할 것이다. URL만 입력하지만, 카드에서는 페이지의 이미지와 설명까지 딸려오며, 이것은 웹 스크래핑(크롤링)으로 가능한 것이다. 
<br>

먼저 새로운 프로젝트 폴더를 만들고(venv 체크!), 'templates', 'static' 폴더와 'app.py' 파일, 그리고 'templates' 폴더 안에 'index.html' 파일을 생성한다. 설정 > 프로젝트 > Python 인터프리터에 들어가 'flask', 'pymongo', 웹 크롤링에 필요한 'requests', 'bs4'를 설치한다.

<br>

## 2. API 설계하기

> 프로젝트를 시작하기 전, 만들고자 하는 서비스에 어떤 기능이 필요하고 그 기능을 어떠한 순서로 구현할 것인지 계획하는 설계 과정이 필요하다. 

'나홀로 링크 메모장'에 필요한 기능은 두 가지다. 첫 번째로, 클라이언트가 URL과 코멘트를 서버로 보내주고 서버는 그 데이터를 DB에 저장한다. 두 번째로, 서버에서 클라이언트로 데이터를 내려주고, 클라이언트는 화면에 카드들을 보여준다. DB에서는 이미지, 제목, 설명, 코멘트 데이터를 저장해야 한다. 카드에서 영화 제목을 클릭하면 해당 링크로 이동하기 때문에, 링크도 저장해 두어야 한다. 

<br>

먼저, 카드를 생성하는 API를 설계해보자. 설계는 **요청 정보(URL, 요청 방식, 요청 데이터 종류와 변수명) / 서버가 제공할 기능 / 응답 데이터**로 구성된다. 

### 1) 포스팅 API 설계(POST)

**① 요청 정보**

DB에 *새로운 데이터를 생성(create)*하는 것이기 때문에 POST 요청을 이용한다. URL은 '/memo'로, `url_give`로 URL 데이터를, `comment_give`로 코멘트 데이터를 받는다.

<br>

**② 서버가 제공할 기능**

서버는 클라이언트 쪽에서 받아온 URL의 메타 태그 정보를 바탕으로 제목, 설명, 이미지 주소(URL)를 스크래핑한다. 그 다음 스크래핑한 데이터를 모두 DB에 저장한다.

<br>

**③ 응답 데이터**

서버가 데이터를 DB에 잘 저장했다면, 클라이언트에게 API가 정상적으로 작동함을 알리는 success 메세지를 준다. 메세지는 JSON 형식으로 `'result' = 'success'`를 입력한다.

<br>

### 2) 리스팅 API 설계(GET)

**① 요청 정보**

저장된 카드를 *보여주는(read)* 것이기 때문에 GET 요청을 사용한다. 포스팅 API와 같은 '/memo' URL을 사용하고, 클라이언트 쪽에서 받는 데이터 없이, DB에 저장된 데이터를 조회하는 것이기 때문에 요청할 데이터는 없다.

<br>

**② 서버가 제공할 기능**

서버는 DB에 저장되어 있는 모든 제목, 설명, 링크, 이미지 주소, 코멘트 정보를 가져온다.

<br>

**③ 응답 데이터**

각 기사들의 정보(제목, 설명, 링크, 이미지 주소, 코멘트)로 카드를 만들어서 붙여야 한다. JSON 형식으로 `'articles': 기사 정보`를 입력한다. 이 리스팅 API는 클라이언트에서 로딩되자마자 호출된다는 것도 기억하자. 

<br>

'기사 저장' 버튼에 Ajax 콜, 로딩되자마자 호출하는 것에 Ajax 콜을 만든다. 복습 차원에서, Ajax는 자바스크립트를 이용해 페이지 전환 없이 서버에서 값을 받아오는 방법이다. [2주차 Ajax 관련 개발일지][2]를 참고하자.

<br>

### 3) 조각 기능 구현하기

API를 설계한 다음, 구현하고자 하는 기능을 조각내어 시험해보는 단계를 거친다. 예를 들어, URL 주소의 이미지와 제목, 요약을 자동으로 가져오는 API를 구상했다면, 설계가 잘 되었는지 검증하는 과정이 필요하다. 프로젝트를 시작하기 전 별개의 파일을 만들어 API에서 수행할 기능들을 미리 테스트하는데, 이를 **조각 기능**이라고 한다. 프로젝트 실전에서 코드를 짜기보다는, 별개의 파일에서 실행해본 후 코드를 붙여넣는 방식이 편하다.

<br>

#### ① meta 태그

입력한 URL 주소의 이미지, 제목, 요약을 크롤링하는 기능을 조각 기능으로 구현해 보자. 이는 meta 태그 스크래핑을 통해 구현한다. **meta 태그**는 아래 사진과 같이 HTML 문서의 `<head>` 태그 안에 위치하며,눈으로 보이는 `<body>` 태그 이외에 사이트의 속성을 설명해주는 태그들이다. HTML을 공부할 때, 사이트가 검색되었을 때 뜨는 제목과 설명 등을 입력하는 태그라고 배운 적이 있다. 이처럼 구글 검색 시 표시되는 설명문, 사이트 제목, 카톡 공유 시 표시되는 이미지 등이 모두 `<meta>` 태그에 포함된다. 

![web403 meta tag](../../assets/images/w04_linkmemo01.jpg)
<br>

#### ② og:title, og:image, og:description

태그를 살펴보면, `og:title`, `og:image`, `og:description`의 property를 가진 `<meta>` 태그들이 각각 영화 제목, 이미지 주소와 요약 데이터를 가지고 있다. 각각의 데이터를 위 속성을 가진 메타 태그에 넣어놓으면, 어느 사이트에서든지 자동으로 해당 데이터를 크롤링해갈 수 있다. 카카오톡과 같은 여러 메신저들에서는 이 속성들을 가진 meta 태그를 불러와서 이미지와 제목, 요약을 보여준다. 내가 URL을 만드는 입장이라면, `og:title`, `og:image`, `og:description`에 데이터를 잘 넣어두어야 해당 링크를 공유할 때 제목과 이미지, 요약이 잘 표시될 것이다. 

<br>

#### ③ meta 태그 크롤링하기

프로젝트 폴더인 'alonememo'에 조각 기능 구현을 위한 `test.py` 파일을 생성한다. 이 파일에서 먼저 크롤링을 연습한 다음, 실제 `app.py`에 필요한 부분을 가져다 쓴다. 앞서 말했듯, 주요 기능은 일단 작동되도록 한 다음 실제 프로젝트에 써먹는 것이 편하다. 실제 크롤링에 앞서, 3주차에 배운 [크롤링 관련 개발일지][3]를 읽고 복습해보자.

<br>

```py
# 크롤링 기본 코드
import requests
from bs4 import BeautifulSoup

url = 'https://movie.naver.com/movie/bi/mi/basic.nhn?code=171539'

headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}
data = requests.get(url,headers=headers)

soup = BeautifulSoup(data.text, 'html.parser')
```
시작 전, 위의 크롤링 기본 코드를 붙여넣는다. `url`에는 영화 '그린북'의 네이버 영화 페이지를 예시 주소로 입력했다. 변수 `url`은 `requests.get`을 통해 `data` 변수로 들어간다. 코드 작성에 앞서 `print(soup)`를 입력 후 실행, HTML 코드가 잘 찍히는지를 확인한다.

<br>

```py
title = soup.select_one('#content > div.article > div.mv_info_area > div.mv_info > h3 > a')

print(title)
# <a href="./basic.naver?code=171539">그린 북</a>
print(title.text)
# 그린 북
```
영화 제목을 크롤링하기 위해, 사이트에서 영화 제목 우클릭 > 검사 > Copy > Copy Selector를 한다. `select_one`을 사용해 선택자(selector)를 붙여넣고 이를 출력하면 해당 태그가 찍힌다. 태그의 텍스트만을 얻고 싶다면 `.text`, 속성을 얻고 싶다면 `['속성']`을 사용한다. 

<br>

```py
title = soup.select_one('head > meta:nth-child(9)')
print(title)
# None
```
하지만, 위 코드처럼 `og:title` 속성의 `<meta>` 태그 선택자를 복사해 출력해보면 'None'이 뜬다. 그 이유는, 브라우저에서 console 창에 접속했을 때 나오는 `<meta>` 태그의 순서와 파이썬 코드에서 접속했을 때의 `<meta>` 태그의 순서가 다르기 때문이다. 








[1]: https://yendoz.github.io/spartaweb/nbw3-1/#%EB%B3%B5%EC%8A%B5-%EB%82%98%ED%99%80%EB%A1%9C-%EB%A9%94%EB%AA%A8%EC%9E%A5%EC%97%90-openapi-%EB%B6%99%EC%97%AC%EB%B3%B4%EA%B8%B0
[2]: https://yendoz.github.io/spartaweb/nbw2-2/#1-json
[3]: https://yendoz.github.io/spartaweb/nbw3-2/#2-%ED%81%AC%EB%A1%A4%EB%A7%81%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%9D%B4%EB%A3%A8%EC%96%B4%EC%A7%80%EB%82%98